<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Lalit Maganti</title>
    <link>/</link>
    <description>Recent content on Lalit Maganti</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sun, 12 Oct 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>What Makes a Good Tool for Claude Code</title>
      <link>/writing-tools-for-claude-code/</link>
      <pubDate>Sun, 12 Oct 2025 00:00:00 +0000</pubDate>
      <guid>/writing-tools-for-claude-code/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been using Claude Code extensively for personal projects, and similar AI&#xA;coding tools at work. Recently I came across&#xA;&lt;a href=&#34;https://www.alephic.com/writing/the-magic-of-claude-code&#34;&gt;this excellent blog post&lt;/a&gt;&#xA;that resonated with a lot of my experience.&lt;/p&gt;&#xA;&lt;p&gt;One part stuck with me though: Noah emphasizes that tools fail with LLMs when&#xA;they&amp;rsquo;re &amp;ldquo;overly complex,&amp;rdquo; with the Unix philosophy being particularly&#xA;well-suited for tool calling. But then I thought about &lt;code&gt;git&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Git breaks the Unix philosophy completely. It&amp;rsquo;s sprawling, stateful, and&#xA;complex. And yet Claude Code handles it effortlessly. It composes commands that,&#xA;even after 10+ years of daily git usage, I wouldn&amp;rsquo;t think to use. It handles&#xA;rebasing, cherry-picking, complex resetsâ€”stuff that trips up experienced&#xA;developers regularly.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Harnessing Frustration: Using LLMs to Overcome Activation Energy</title>
      <link>/llm-motivation-via-emotions/</link>
      <pubDate>Mon, 14 Jul 2025 00:00:00 +0000</pubDate>
      <guid>/llm-motivation-via-emotions/</guid>
      <description>&lt;p&gt;One of my biggest weaknesses as a software engineer is procrastination&#xA;when facing a new project. When the scope is unclear, I have a tendency to&#xA;wait until I feel I&amp;rsquo;ve &amp;ldquo;felt out&amp;rdquo; the problem to start doing anything.&#xA;I know I&amp;rsquo;ll feel better and work much faster when I get &amp;ldquo;stuck in&amp;rdquo; but&#xA;I still struggle with that first step, overcoming the &amp;ldquo;activation energy&amp;rdquo;&#xA;required to engage with the details.&lt;/p&gt;&#xA;&lt;p&gt;LLMs have been a game-changer for me in this respect: I can just throw a couple&#xA;of sentences at them with the shape of the problem. This leads to one of two&#xA;outcomes:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The LLM comes up with a good solution, usually in a slightly different&#xA;way than what I was thinking. I realize &amp;ldquo;oh wow the solution is much&#xA;simpler than I thought&amp;rdquo;. Straight away I start thinking about the consequences&#xA;of implementing and improving what the LLM suggested.&lt;/li&gt;&#xA;&lt;li&gt;The LLM comes up with a solution that I intuitively recognize as &amp;ldquo;wrong&amp;rdquo;.&#xA;My immediate reaction is frustration (&amp;ldquo;How could it get it so wrong&amp;rdquo;) which leads me&#xA;to go back and forth with the model, explaining to it why its solution&#xA;could not &lt;em&gt;possibly&lt;/em&gt; work. But in the process of arguing with the model, my&#xA;brain is churning away and generating variations or different&#xA;approaches that &lt;em&gt;could&lt;/em&gt; work. After a while, even if the AI is still on the wrong&#xA;track, the debate will trigger a moment of inspiration where suddenly the solution will&#xA;come to me. I&amp;rsquo;ll excitedly start up a new conversation and start working through&#xA;it with the model.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;The key is the emotional reaction I have immediately to the LLM&amp;rsquo;s response, either&#xA;&lt;strong&gt;excitement or frustration&lt;/strong&gt;. By harnessing this immediate feedback loop, I&#xA;get my brain out of its passive, procrastination mode. It&amp;rsquo;s almost like a jolt:&#xA;either I&amp;rsquo;m thrilled because it&amp;rsquo;s simpler than I thought, or I&amp;rsquo;m spurred to action&#xA;by the urge to correct a perceived &amp;lsquo;wrong&amp;rsquo; answer. This forces me to engage with&#xA;the problem in a meaningful way.&lt;/p&gt;&#xA;&lt;p&gt;For what it&amp;rsquo;s worth, this experience is very similar to talking through a problem with&#xA;another engineer: the advantage of LLMs is that it&amp;rsquo;s available 24/7 and I never have to worry&#xA;about my problem being &amp;ldquo;too insignificant&amp;rdquo; to bother someone with.&#xA;The simple act of articulating my thoughts and hearing a response seems sufficient,&#xA;regardless of the response&amp;rsquo;s quality.&lt;/p&gt;</description>
    </item>
    <item>
      <title>V4L2 and Hardware Encoding on the Raspberry Pi</title>
      <link>/hw-encoding-raspi/</link>
      <pubDate>Mon, 01 Feb 2021 23:00:00 +0000</pubDate>
      <guid>/hw-encoding-raspi/</guid>
      <description>&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;: Explain how the V4L2 M2M API works through the use-case of implementing hardware video encoding on&#xA;the Raspberry Pi. This knowledge is generally useful as V4L2 is the de-facto generic API for hardware decoding&#xA;and encoding on Linux.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;My journey started at &lt;a href=&#34;https://www.youtube.com/watch?v=CyEpshm16HY&amp;amp;t=139s&#34;&gt;this video&lt;/a&gt;&#xA;on the excellent &lt;a href=&#34;https://www.youtube.com/channel/UCp3yVOm6A55nx65STpm3tXQ&#34;&gt;Craft Computing&lt;/a&gt; YouTube channel&#xA;which showed how to setup TinyPilot, a Python app for KVM over IP which runs on a Raspberry Pi.&#xA;Behind the scenes, TinyPilot uses &lt;a href=&#34;https://github.com/pikvm/ustreamer&#34;&gt;ustreamer&lt;/a&gt; to read frames&#xA;from a HDMI capture card and either exposes it over HTTP or writes it to shared memory. Along with the&#xA;MJPEG output, support was recently added for encoding video using H264.&lt;/p&gt;&#xA;&lt;p&gt;Even after messing with the source code, I could not get the H264 encoding working on my Pi running&#xA;64-bit Ubuntu with an error message of &lt;code&gt;Can&#39;t create MMAL wrapper&lt;/code&gt;. Digging further, I ran into some&#xA;insurmountable roadblocks with the approach taken by ustreamer and discovered the complex state of&#xA;hardware encoding on the Pi.&lt;/p&gt;&#xA;&lt;p&gt;In short, there are three userspace APIs for accessing the HW encoding and decoding on the&#xA;Pi&amp;rsquo;s Broadcom chip:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;OpenMAX Integration Layer (OpenMAX IL)&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;This API came directly from Broadcom and was the original way to access hardware codecs.&lt;/li&gt;&#xA;&lt;li&gt;Support was dropped by Broadcom a long time ago and there appears to be&#xA;&lt;a href=&#34;https://github.com/raspberrypi/Raspberry-Pi-OS-64bit/issues/98#issuecomment-715321951&#34;&gt;no hope of it&lt;/a&gt;&#xA;ever working on 64-bit OSes.&lt;/li&gt;&#xA;&lt;li&gt;ustreamer &lt;a href=&#34;https://github.com/pikvm/ustreamer/blob/master/src/ustreamer/encoders/omx/encoder.c&#34;&gt;uses this API&lt;/a&gt;&#xA;to perform MJPEG encoding when specifically requested by the user.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Multimedia Abstraction Layer (MMAL)&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;This API was developed by the Pi developers as a replacement for OpenMAX and backs tools like&#xA;&lt;code&gt;raspistill&lt;/code&gt; and &lt;code&gt;raspivid&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;While support for 64-bit was added&#xA;&lt;a href=&#34;https://github.com/raspberrypi/userland/commit/7d3c6b9f4c3ddeecefdeb2b882bada74a235249b&#34;&gt;at one point&lt;/a&gt;,&#xA;it was subsequently&#xA;&lt;a href=&#34;https://github.com/raspberrypi/userland/commit/f97b1af1b3e653f9da2c1a3643479bfd469e3b74&#34;&gt;reverted&lt;/a&gt; and,&#xA;currently, does not work.&lt;/li&gt;&#xA;&lt;li&gt;ustreamer uses &lt;a href=&#34;https://github.com/pikvm/ustreamer/blob/master/src/ustreamer/h264/encoder.c&#34;&gt;this API&lt;/a&gt;&#xA;to perform H264 encoding to shared memory.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;Video4Linux2 Memory to Memory (V4L2 M2M)&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;V4L2 is Linux API which was historically used for video capture and output. In 2010, the M2M API&#xA;was added, significantly extending V4L2 and adding support for video codecs.&lt;/li&gt;&#xA;&lt;li&gt;The Raspberry Pi kernel &lt;a href=&#34;https://github.com/raspberrypi/linux/blob/rpi-5.11.y/drivers/staging/vc04_services/bcm2835-codec/bcm2835-v4l2-codec.c&#34;&gt;has a driver&lt;/a&gt;&#xA;which uses MMAL APIs in kernelspace and integrates with the rest of V4L2.&lt;/li&gt;&#xA;&lt;li&gt;Since this is just a generic kernel API, it works out of the box on 64-bit OSes with programs&#xA;including ffmpeg using it.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Since digging into the internals of MMAL seemed complex, even to knowledgable folks in the Pi community,&#xA;I decided to try to moving ustreamer&amp;rsquo;s H264 encoding to use V4L2 instead. As a side benefit, since we&amp;rsquo;re&#xA;using Linux kernel APIs, this should also futureproof this code to any new revisions of the Pi.&lt;/p&gt;&#xA;&lt;p&gt;I quickly ran into the fact that, while lots of people have used V42L for cameras and webcams,&#xA;only the big projects like GStreamer and ffmpeg use it for encoding. This meant there was no simple guide&#xA;or example I could follow. However, with a lot of poring over kernel documentation and&#xA;trial and error, I learnt enough to successfully perform H264 hardware encoding using V4L2 and&#xA;&lt;a href=&#34;https://github.com/LalitMaganti/ustreamer/commit/a14822a03da74c429928060d99a06e7a0a39d030&#34;&gt;modified ustreamer&lt;/a&gt;&#xA;to support it.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Using V4L2: step by step&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Generally, V4L2 works in either &amp;ldquo;capture&amp;rdquo; (frames being read off a device e.g. camera or webcam) or&#xA;&amp;ldquo;output&amp;rdquo; (frames being sent to a device e.g. a graphics card) M2M combines &amp;ldquo;capture&amp;rdquo; and&#xA;&amp;ldquo;output&amp;rdquo; into a single mode which allows arbitrary transformations of frames. Video encoding is the&#xA;obvious example of this but decoding is also supported.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;/img/hw-encoding-raspi/v4l2.png&#34; alt=&#34;Diagram for V4L2 APIs&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;All communication with the hardware encoder on the Pi is done by opening a special device in&#xA;&lt;code&gt;/dev&lt;/code&gt; (&lt;code&gt;/dev/video11&lt;/code&gt; to be precise), performing &lt;code&gt;ioctls&lt;/code&gt; on the file descriptor to control the&#xA;encoding and using mmap-ed memory to pass the frames between userspace and kernelspace.&lt;/p&gt;&#xA;&lt;p&gt;There are two phases to the encoding process: setup and streaming. In the setup phase, we tell the encoder&#xA;about things like the pixel format, resolution, frame rate and setup the buffers to pass the frames.&#xA;The streaming phase involves writing the next raw frame of data to the kernel and reading back the&#xA;encoded frame.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;Note: all error handling is omitted to make the code snippets smaller.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;First, we open the special device for the video encoder. This gives an fd we can use for &lt;code&gt;ioctls&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;linux/videodev2.h&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; fd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/dev/video11&amp;#34;&lt;/span&gt;, O_RDWR);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next, we setup the &amp;ldquo;output&amp;rdquo; and &amp;ldquo;capture&amp;rdquo; devices; the most important things to set are resolution&#xA;and pixel format. The &amp;ldquo;capture&amp;rdquo; device will be implicitly set to produce H264 frames with the same&#xA;pixel format as the output but the resolution needs to be set manually.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;Note: confusingly &amp;ldquo;output&amp;rdquo; here refers to the raw frames being encoded and &amp;ldquo;capture&amp;rdquo; to the encoded H264&#xA;output frames. A good way to think about this is to consider how V4L2 was originally designed:&#xA;frames are sent to output devices like graphics cards and read from capture devices like cameras.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;For 1080p raw frames in &lt;a href=&#34;&#34;&gt;YUYV format&lt;/a&gt;:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;v4l2_format&lt;/span&gt; fm;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;v4l2_pix_format_mplane&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;mp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;fm.fmt.pix_mp;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;fm.type &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ioctl(fd, VIDIOC_G_FMT, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;fm);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mp&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;width &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1920&lt;/span&gt;;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mp&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;height &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1080&lt;/span&gt;;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mp&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;pixelformat &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; V4L2_PIX_FMT_YUYV;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ioctl(fd, VIDIOC_S_FMT, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;fm);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;fm.type &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ioctl(fd, VIDIOC_G_FMT, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;fm);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mp&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;width &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1920&lt;/span&gt;;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mp&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;height &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1080&lt;/span&gt;;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ioctl(fd, VIDIOC_S_FMT, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;fm);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now, we set the inter-frame interval to indirectly set the frame rate. For a 30FPS video:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;v4l2_streamparm&lt;/span&gt; stream;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;memset(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;stream, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;sizeof&lt;/span&gt;(stream));&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;stream.type &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;stream.parm.output.timeperframe.numerator &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;stream.parm.output.timeperframe.denominator &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ioctl(fd, VIDIOC_S_PARM, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;stream);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The most complex part of the setup is configuring the mmap buffers to send and receive frames.&#xA;Since we&amp;rsquo;re not trying to be performant, we use one capture buffer and one output buffer.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;buffer&lt;/span&gt; {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; start;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; length;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;v4l2_buffer&lt;/span&gt; inner;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;v4l2_plane&lt;/span&gt; plane;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// mmaps the buffers for the given type of device (capture or output).&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;map&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; fd, &lt;span style=&#34;color:#66d9ef&#34;&gt;uint32_t&lt;/span&gt; type, &lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;buffer&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; buffer) {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;v4l2_buffer&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;inner &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;buffer&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;inner;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  memset(inner, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;sizeof&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;inner));&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  inner&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;type &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; type;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  inner&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;memory &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; V4L2_MEMORY_MMAP;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  inner&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  inner&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;length &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  inner&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;m.planes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;buffer&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;plane;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  ioctl(fd, VIDIOC_QUERYBUF, inner);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  buffer&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;length &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; inner&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;m.planes[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;].length;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  buffer&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;start &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mmap(NULL, buffer&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;length, PROT_READ &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; PROT_WRITE,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      MAP_SHARED, fd, inner&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;m.planes[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;].m.mem_offset);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;v4l2_requestbuffers&lt;/span&gt; buf;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;buf.memory &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; V4L2_MEMORY_MMAP;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;buf.count &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;buffer&lt;/span&gt; output;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;buf.type &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ioctl(fd, VIDIOC_REQBUFS, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;buf);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;map(fd, V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;output);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;buffer&lt;/span&gt; capture;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;buf.type &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ioctl(fd, VIDIOC_REQBUFS, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;buf);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;map(fd, V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;capture);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next, we &amp;ldquo;queue&amp;rdquo; up the capture and output buffers. This tells the encoder that it has exclusive&#xA;access to these buffers until we dequeue them.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ioctl(fd, VIDIOC_QBUF, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;capture&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;inner);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ioctl(fd, VIDIOC_QBUF, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;output&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;inner);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Finally, we start the streaming process; this tells the kernel that it should start&#xA;reading raw frames from the output buffer and writing encoded frames to the capture buffer:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; type &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ioctl(fd, VIDIOC_STREAMON, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;type);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;type &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ioctl(fd, VIDIOC_STREAMON, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;type);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    <item>
      <title>About</title>
      <link>/page/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/page/about/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m a software engineer working at Google on &lt;a href=&#34;https://perfetto.dev&#34;&gt;Perfetto&lt;/a&gt;, a suite of open source tracing tools helping developers understand the behaviour and performance of complex systems. This includes both Android and Chrome where Perfetto is the default tracing system. Perfetto is also used by many other products in Google and by other companies in the industry. My main work is with the&#xA;&lt;a href=&#34;https://perfetto.dev/docs/analysis/trace-processor&#34;&gt;trace processor&lt;/a&gt;, a C++ library that imports&#xA;traces in a wide variety of formats and exposes an SQL interface for querying their contents.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
